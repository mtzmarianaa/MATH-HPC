
\documentclass[11pt]{article}
\usepackage{../../Shared_Resources/Latex_Styles/General_Style} 
\usepackage{../../Shared_Resources/Latex_Styles/mcode} 

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\newcommand\bigO[1]{{\ensuremath{\mathcal{O}(#1)}}}
\newtheorem{theorem}{Theorem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\Input{\item[\algorithmicinput]}%
\algnewcommand\Output{\item[\algorithmicoutput]}%

\usepackage{listings}

\lstset{ frame=single}

\begin{document}

\lstset{frameround=fttt,language=Matlab}

\lstMakeShortInline[columns=fixed]|

\makeheader{7 -- October 31, 2023}{Randomized SVD}


{\bf{Exercise 1: SRHT}} \\

In the context of overdetermined least-squares problems, we need to find $x \in \R^n$ such that it minimizes:

\[ \|Wx - b\|_2^2, \]

where $W \in \R^{m \times n}, b \in \R^{m}, m > n$. There is a class of randomized algorithms for solving this problem based on sketching method. Sketching methods involve using a random matrix $\Omega \in \R^{l \times m}$ to project the data $W$ (and maybe also $b$) to a lower dimensional space with $l \ll m$. Then they approximately solve the least-squares problem using the sketch $\Omega W$ (and/or $\Omega b$).  One relaxes the problem to finding a vector $x$ so that 

\[ \|Wx - b\| \leq (1 + \varepsilon)\|Wx^* - b\|, \]

where $x^*$ is the optimal solution. The overview of sketching applied to solve linear least squares is:

\begin{enumerate}
    \item Sample/build a random matrix $\Omega$
    \item Compute $\Omega A$ and $\Omega b$
    \item Output the exact solution to the problem $\min_{x} \| (\Omega W)x - (\Omega) b\|_2$.
\end{enumerate}

Given a data matrix, $W \in \R^{m \times n}$, we want to reduce the dimensionality of $W$ by defining a random orthonormal matrix $\Omega \in \R^{l \times m}$ with $l \ll m$. For $m = 2^q, q \in \mathbb{N}$, the Subsampled Randomized Hadamard Transform (SRHT) algorithm defined a $l \times m$ matrix as:

\[ \Omega = \sqrt{\frac{m}{l}} PH_{m}D, \]

where:

\begin{itemize}
    \item $D \in \R^{m \times m}$ is a diagonal matrix whose elements are independent random signs, i.e. it's diagonal entries are just $-1$ or $1$.
    \item $H \in \R^{m \times m}$ is a \textbf{normalized} Walsh-Hadamard matrix. If you're going to use a library that implements this transform then check that it implements the normalized Walsh-Hadamard matrix. This matrix is defined recursively as:
    \begin{align*}
    H_m &= \begin{bmatrix} H_{m/2} & H_{m/2} \\ H_{m/2} & -H_{m/2} \end{bmatrix} & H_2 &= \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \\
    H &= \frac{1}{\sqrt{m}} H_m \in \R^{m \times m}.
    \end{align*}
    \item $P \in \R^{l \times m}$ is a subset of randomly sampled $l$ columns from the $m \times m$ identity matrix. The purpose of using $P$ is to uniformly sample $r$ columns from the rotated data matrix $X_{\text{rot}} = H_{m}DX$.
\end{itemize}

The following theorem help us get an idea for the size of $l$.

\begin{theorem}[Subsampled Randomized Hadamard Transform]
Let $\Omega = \sqrt{\frac{m}{l}}PH_{m}D$ as previously defined. Then if

\[ l \geq \bigO{( \varepsilon^{-2}\log(n)) (\sqrt{n} + \sqrt{\log{m}})^2 }\]

with probability $0,99$ for any fixed $U \in \R^{m \times n}$ with orthonormal columns:

\[ \| I - U^\top \Omega \Omega^\top U\|_2 \leq \varepsilon. \]

Further, for any vector $x \in \R^{m}, \Omega x$ can be computed in $\bigO{n \log{l}}$ time.

\end{theorem}

Choose a data set from \href{https://www.kaggle.com/datasets?tags=13405-Linear+Regression}[https://www.kaggle.com/datasets?tags=13405-Linear+Regression]. Compare the randomized least squares fit using SRHT vs the deterministic least squares fit. Use the previous theorem to estimate $l$. \textit{Hint: you can use the fast Hadamard transform from scipy or pytorch}

\bigskip

{\bf{Exercise 2: Randomized SVD}}

Consider the following algorithm to compute a randomized SVD factorization:

\scriptsize
\begin{algorithm}
\caption{Randomized SVD q = 1}\label{Randomized SVD}
\begin{algorithmic}
\Input $A \in \R^{m \times n}$, desired rank $k$, $l = p+k$
\Output Approximation $A_k = Q_1U, \Sigma, V$
\State Sample an $n \times l$ test matrix $\Omega_1$ with intependend mean-zero, unit-variance Gaussian entries.
\State Compute $Y = (AA^{\top})A\Omega_1$
\State Construct $Q_1 \in \R^{m \times l}$ with columns forming an orthonormal basis for the range of $Y$.
\State Compute $B = Q_1^{\top} A, B \in R^{l \times n}$
\State Compute the rank-k truncated SVD of $B$ as $U\Sigma V^{\top}, U \in \R^{l \times k}, V\in \R^{n \times k}$
\end{algorithmic}
\end{algorithm}
\normalsize

Remember the following theorem:

\begin{theorem}
If $\Omega_1$ is chosen to be i.i.d. $\mathcal{N}(0, 1)$, $k, p \geq 2$, then the expectation with respect to the random matrix $\Omega_1$ is:

\[ \mathbb{E}(\|A - Q_1Q_1^{\top}A\|_2) \leq \left( 1 + \frac{4 \sqrt{k + p}}{p - 1} \sqrt{\min(m, n)} \right) \sigma_{k+1}(A) \]

and the probability that the error satisfies

\[ \|A - Q_1Q_1^{\top}A\|_2 \leq \left( 1 + 11 \sqrt{k + p} \sqrt{\min(m, n)} \right)\sigma_{k+1}(A) \]

is at least $1 - 6/p^p$. For $p = 6$, the probability becomes $0,99$.

\end{theorem}

Construct a rank$-k$ approximation with $k = 10, p=6$ to a matrix $A \in \R^{m \times 2m}$ via its SVD:

\[ A = U^{(A)} \Sigma^{(A)} V^{(A)\top}, \]

where:

\begin{itemize}
    \item $U \in \R^{m \times m}$ is a Hadamard matrix
    \item $V \in \R^{2m \times 2m}$ is a Hadamard matrix
    \item $\Sigma \in \R^{m \times 2m}$ is a diagonal matrix whose diagonal entries are defined as:
    \[ \Sigma_{jj} = \sigma_j = (\sigma_{k+1})^{\lfloor j/2 \rfloor / 5}, \]
    for $j = 1, 2, ..., 9, 10$ and
    \[ \Sigma_{jj} = \sigma_j = \sigma_{k+1} \frac{m-j}{m-11}, \]
    for $j = 11, 12, ..., m-1, m$. Thus $\sigma_1 = 1$ and $\sigma_k = \sigma_{k+1}$.
\end{itemize}

Test this algorithm for $m = 2^{11}$, $\sigma_{k+1} = 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001$. Plot the decay of the singular values of $A$ and compare such decay with the accuracy of the approximation,  $ \| A - Q_1Q_1^{\top}A\|_2$. Compare it with the theorem presented above.

\bibliographystyle{elsarticle-num-names}
\bibliography{ref}

\end{document}
