
\documentclass[11pt]{article}
\usepackage{../../Shared_Resources/Latex_Styles/General_Style} 
\usepackage{../../Shared_Resources/Latex_Styles/mcode} 

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\newcommand\bigO[1]{{\ensuremath{\mathcal{O}(#1)}}}
\newtheorem{theorem}{Theorem}

\usepackage{listings}

\lstset{ frame=single}

\begin{document}

\lstset{frameround=fttt,language=Matlab}

\lstMakeShortInline[columns=fixed]|

\makeheader{6 -- October 24, 2023}{Sketching techniques}

In the context of overdetermined least-squares problems, we need to find $x \in \R^n$ such that it minimizes:

\[ \|Ax - b\|_2^2, \]

where $A \in \R^{m \times n}, b \in \R^{m}, m > n$. There is a class of randomized algorithms for solving this problem based on sketching method. Sketching methods involve using a random matrix $\Omega \in \R^{r \times m}$ to project the data $A$ (and maybe also $b$) to a lower dimensional space with $r \ll m$. Then they approximately solve the least-squares problem using the sketch $\Omega A$ (and/or $\Omega b$).  One relaxes the problem to finding a vector $x$ so that 

\[ \|Ax - b\| \leq (1 + \varepsilon)\|Ax^* - b\|, \]

where $x^*$ is the optimal solution. The overview of sketching applied to solve linear least squares is:

\begin{enumerate}
    \item Sample/build a random matrix $\Omega$
    \item Compute $\Omega A$ and $\Omega b$
    \item Output the exact solution to the problem $\min_{x} \| (\Omega A) - (\Omega) b\|_2$.
\end{enumerate}

{\bf{Exercise 1: General properties of sketching techniques}}\\

\begin{enumerate}
    \item A $(1 \pm \varepsilon) \; l_2-$subspace embedding for the column space of a $m \times n$ matrix $A$ is a matrix $\Omega$ for which for all $x \in \R^{n}$ the following property is satisfied:
    
    \begin{align}
    \label{def:eps embedding}
    \|\Omega A x\|_2^2 = (1 \pm \varepsilon)\|A x\|_2^2.
    \end{align}

Let $U$ be a matrix whose columns form an orthonormal basis for the column space of $A$. Prove that the requirement of an $(1 \pm \varepsilon) \; l_2-$subspace embedding can be simplified to:

\[ \|I - U^\top S^\top S U \|_2 \leq \epsilon. \] 

\textbf{Solution: } if $s$ is the rank of $A$ then the following sets are equal:

\[ \{Uy : y \in \R^s\} = \{ Ax : x \in \R^n \}. \]

Hence, without loss of generality we can assume that $A$ has orthonormal columns. Then the requirement for a $(1 \pm \varepsilon) \; l_2-$subspace embedding becomes:

\[ \|SUy\|_2^2 = (1 \pm \varepsilon )\|Uy\|_2^2 = (1 \pm \varepsilon) \|y\|_2^2. \]

If this requirement is satisfied for unit vectors $y$, then it is satisfied for all vectors $y$ by scaling. Then:

\begin{align*}
\|I - U^\top S^\top S U \|_2 &= \max\{ \| (I - U^\top S^\top S U) y \|_2 : \|y\|_2 = 1 \} \leq \varepsilon
\end{align*}

\item Let $g_1, ..., g_t$ be i.i.d. $\mathcal{N}(0,1)$ random variables. Then for any $x \geq 0$:

\begin{align*}
P\left( \sum_{i = 1}^t g_i^2 \geq t + 2 \sqrt{tx} + 2x \right) &\leq e^{-x} \\
P\left( \sum_{i = 1}^t g_i^2 \leq t - 2 \sqrt{tx} \right) &\leq e^{-x}
\end{align*}

Now prove the following theorem:

\begin{theorem}[Johnson-Lindenstrauss]
\label{Lemma18}
Given $n$ points $q_1, ..., q_n \in \R^m$ if $G$ is a $t \times m$ matrix of i.i.d. $\mathcal{N}(0, 1/t)$ random variables, then for $t = \bigO{\log(n/\varepsilon^3)}$ simultaneously for all $i \in 1, ..., n$:

\[ P\left( \|Gq_i\|_2 \in (1 \pm \varepsilon) \|q_i\|_2 \right) \geq 1 - \frac{1}{n}. \]
 
\end{theorem}

\textbf{Solution: } For a fixed $i$, we know that $Gq_i$ is a t-tuple of i.i.d. $\mathcal{N}(0, \|q_i\|_2^2/t)$ random variables. It follows that $\|Gq_i\|_2^2$ is equal in distribution to $(\|q_i\|_2^2/t) \sum_{i = 1}^t g_i^2$, where $g_1, ..., g_t$ are independent standard normal random variables. We also know that the random variable $\sum_{i = 1}^t g_i^2$ is a $\chi^2$ with $t$ degrees of freedom. Then using the bounds provided we set $x = \varepsilon^2 t/16$ to get 

\[ P\left( \left\lvert \sum_{i = 1}^t g_i^2 - t \right\rvert \leq \varepsilon t \right) \leq 2 e^{\varepsilon^2 t/16}. \]

Then for $t = \bigO{\log(n/\varepsilon^2)}$ and by the union bound over $i$ (Boole's theorem) we get that all $i$:

\[  P\left( \|Gq_i\|_2 \in (1 \pm \varepsilon) \|q_i\|_2 \right) \geq 1 - \frac{1}{n}. \]

\end{enumerate}



{\bf{Exercise 2: Gaussian}}\\

The most "classical" sketch is a matrix $\Omega \in \R^{r \times m}$ with independent and identically distributed (i.i.d.) Gaussian entries $\mathcal{N}(0, 1/r)$. The following theorem from [1] provides the optimal number of rows of $\Omega$ up to a constant factor $\bigO{r\epsilon^{-2}}$:

\begin{theorem}
Let $0 < \varepsilon, \delta < 1$ and $\Omega = \frac{1}{\sqrt{r}} R \in \R^{r \times m}$ where the entires $R_{i,j}$ of $R$ are independent standard normal random variables. Then if $r = \bigO{(n + \log(1/\delta))\varepsilon^{-2}}$, then for any fixed $m \times n$ matrix $A$, with probability $1-\delta$, $\Omega$ is a $(1 \pm \varepsilon) \; l_2$-subspace embedding for $A$, that is, simultaneously for all $x \in \R^{n}$,

\begin{align}
\|\Omega A x\|_2 = (1 \pm \varepsilon)\|Ax\|_2
\end{align}

\end{theorem}

Choose a data set from \href{https://www.kaggle.com/datasets?tags=13405-Linear+Regression}[https://www.kaggle.com/datasets?tags=13405-Linear+Regression]. Compare the linear regression obtained from solving the deterministic least squares problem vs the one obtained from the randomized least squares problem with $\Omega \in \R^{r \times m}$ a normal random variable. That is, using the previous theorem with $\delta = 0,99$ choose different values of $\varepsilon$ and compare the difference between the randomized least squares fit vs the deterministic one. Check that (2) holds for every $\varepsilon$ you choose.

The following script does this and generates important plots:

\lstinputlisting{../build/solution/gaussian_sketching.py}
\bigskip

{\bf{Exercise 3: SRHT}}\\

Given a data matrix, $X \in \R^{m \times n}$, we want to reduce the dimensionality of $X$ by defining a random orthonormal matrix $\Omega \in \R^{r \times m}$ with $r \ll m$. For $m = 2^q, q \in \mathbb{N}$, the Subsampled Randomized Hadamard Transform (SRHT) algorithm defined a $r \times m$ matrix as:

\[ \Omega = \sqrt{\frac{m}{r}} PH_{m}D, \]

where:

\begin{itemize}
    \item $D \in \R^{m \times m}$ is a diagonal matrix whose elements are independent random signs, i.e. it's diagonal entries are just $-1$ or $1$.
    \item $H \in \R^{m \times m}$ is a \textbf{normalized} Walsh-Hadamard matrix. If you're going to use a library that implements this transform then check that it implements the normalized Walsh-Hadamard matrix. This matrix is defined recursively as:
    \begin{align*}
    H_m &= \begin{bmatrix} H_{m/2} & H_{m/2} \\ H_{m/2} & -H_{m/2} \end{bmatrix} & H_2 &= \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \\
    H &= \frac{1}{\sqrt{m}} H_m \in \R^{m \times m}.
    \end{align*}
    \item $P \in \R^{r \times m}$ is a subset of randomly sampled $r$ columns from the $m \times m$ identity matrix. The purpose of using $P$ is to uniformly sample $r$ columns from the rotated data matrix $X_{\text{rot}} = H_{m}DX$.
\end{itemize}

The following theorem help us get an idea for the size of $r$.

\begin{theorem}[Subsampled Randomized Hadamard Transform]
Let $\Omega = \sqrt{\frac{m}{r}} PH_{m}D$ as previously defined. Then if

\[ r \geq \bigO{( \varepsilon^{-2}\log(n)) (\sqrt{n} + \sqrt{\log{m}})^2 }\]

with probability $0,99$ for any fixed $U \in \R^{m \times n}$ with orthonormal columns:

\[ \| I - U^\top \Omega \Omega^\top U\|_2 \leq \varepsilon. \]

Further, for any vector $x \in \R^{m}, \Omega x$ can be computed in $\bigO{n \log{r}}$ time.

\end{theorem}

Take the same data set from the previous exercise. Compare the randomized least squares fit using SRHT vs the deterministic least squares fit. Use the previous theorem to estimate $r$. 

\textit{We are going to work on this next week. Solutions will be provided then.}

\bibliographystyle{elsarticle-num-names}
\bibliography{ref}

\end{document}
