# Exericise 4: All-to-all
# Source: https://subscription.packtpub.com/book/programming/9781785289583/3/ch03lvl1sec49/collective-communication-using-alltoall

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

if rank == 0:
    vector = np.arange(size, dype = int)
else:
    vector = None


local_data = comm.alltoall(vector)

print(" process ", rank, " local data ", local_data)

