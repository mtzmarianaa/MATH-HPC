
\documentclass[11pt]{article}
\usepackage{../../Shared_Resources/Latex_Styles/General_Style} 
\usepackage{../../Shared_Resources/Latex_Styles/mcode} 


\usepackage{listings}

\lstset{ frame=single}

\begin{document}

\lstset{frameround=fttt,language=Matlab}

\lstMakeShortInline[columns=fixed]|

\makeheader{5 -- October 17, 2023}{TSQR Factorization}

For this week you can choose what to do: either these exercises or continue working on your project (or maybe even both). 

{\bf{Exercise 1 CGS and MGS}}\\

If we recall what a QR factorization is, given a matrix $W \in \R^{m \times n}$, with $m \geq n$, its QR factorization is

\[ W = QR = \begin{bmatrix} \tilde{Q} & \bar{Q} \end{bmatrix} \begin{bmatrix} \tilde{R} \\ 0 \end{bmatrix} = \tilde{Q}\tilde{R}, \]

where $Q \in \R^{m \times n}$ orthogonal and $R \in \R^{m \times n}$ upper triangular. Note that $W$ can be seen as a map $W: \R^{n} \to \R^{m}$. Recall that computing the QR decomposition using CGS is numerically unstable. An alternative algorithm, which is mathematically equivalent is the Modified Gram-Schmidt algorithm (MGS). Define $Q_{j-1}$ as the matrix we get at the j-th step, $Q_{j-1} = \begin{bmatrix} q_1 & q_2 & \hdots & q_{j-1} \end{bmatrix}$ and $P_j$ as the projector onto the subspace orthogonal to the column space col$(Q_{j-1})$. Then we can write this as:

\[ P_j = I - Q_{j-1}Q_{j-1}^* = (I - q_{j-1}q_{j-1}^*) \hdots  (I - q_{1}q_{1}^*).\]

\begin{enumerate}
   \item How many synchronizations do we need for each vector $w_j$ in this case? Why? \\
   \textit{We need j-1 synchronizations since we need these previous vectors. }
   \item Why is this more stable than CSG? 
   \textit{Intuitively, CSG is more unstable because you subtract off the projections of the (k+1)th vector onto de first k vectors. You ensure that this new vector is orthogonal to the input vector in question but fail to ensure that the vectors you get at the end of the process are orthogonal to each other. So MGS is more stable because you do ensure that the vectors you get at the end of the process are orthogonal to each other.}
   \item Make the necessary modifications to your last week's code to implement MGS. Compare both codes by computing $\|I - \tilde{Q}\tilde{Q}^\top\|$. 
 
   
\end{enumerate}

\bigskip

{\bf{Exercise 2 TSQR}}\\

Remember that communication refers to messages between processors. In the recent years we've seen trends causing floating point to become faster than communication. This is why it's important to minimize communication when dealing with parallelism. The TSQR, "Tall Skinny QR" algorithm is a communication avoiding algorithm for matrices with many more rows than columns. In this exercise we are going to assume we're using $P=4$ processors. The computation can be expressed as a product of intermediate orthonormal factors:

\[ W = \begin{bmatrix} W_1 \\ W_2 \\ W_3 \\ W_4 \end{bmatrix} = \begin{bmatrix} Q_{00} & 0 & 0 & 0 \\ 0 & Q_{10} & 0 & 0 \\ 0 & 0 & Q_{20} & 0 \\ 0 & 0 & 0 & Q_{30} \end{bmatrix} \cdot \begin{bmatrix} Q_{01} & 0 \\ 0 & Q_{11} \end{bmatrix} \cdot Q_{02} \cdot R_{02}. \]

\begin{enumerate}
    \item Let $Q \in \mathbb{C}^{a \times b}$ and $S \in \mathbb{C}^{b \times c}$ be matrices such that their columns form an orthonormal set. Show that the columns of $QS$ also form an orthonormal set. \\
    \textit{Notice that Q is not a square matrix and hence we can't talk about its inverse. What we do know is the following:}
    \[ q_i^\top q_j = \delta_{ij}, \]
    \textit{where} $q_i$  \textit{is the i-th column of Q. We can rewrite this as}:
    \[ Q^\top Q = I. \]
    \textit{Then for } $QS$:
    \[  (QS)^\top(QS) = S^\top Q^\top QS = I.  \]
    \item What are the dimensions of $Q_{i0}, Q_{i1},$ and $Q_{02}$?
    \textit{The dimensions of these matrices depends on whether we use reduced QR factorization of full QR factorization on each intermediate step. If we use reduced QR factorization, then} $Q_{02} \in \R^{2n \times n}, Q_{i1} \in \R^{2n \times n}$, and $Q_{i0} \in \R^{m/4 \times n}$.
    \item Show that the columns of the following matrix are orthonormal. What is the dimension of that matrix?
    \[ \begin{bmatrix} Q_{00} & 0 & 0 & 0 \\ 0 & Q_{10} & 0 & 0 \\ 0 & 0 & Q_{20} & 0 \\ 0 & 0 & 0 & Q_{30} \end{bmatrix} \cdot \begin{bmatrix} Q_{01} & 0 \\ 0 & Q_{11} \end{bmatrix} \cdot Q_{02}  \]
    \textit{Each of these sub blocks is an orthogonal matrix since it was obtained from a QR decomposition. Notice that a diagonal block matrix of orthogonal matrices is orthogonal. Because of the first exercise we have that the product of these matrices is orthogonal as well.}
    \item Suppose that you are given the implicit representation of $\tilde{Q}$ as a product of orthogonal matrices. This representation is a tree of sets of Householder vectors, $\{Q_{r, k}\}$. Here, $r$ indicates the processor
number (both where it is computed and where it is stored), and
$k$ indicates the level in the tree. How would you get $\tilde{Q}$ explicitly? We only need the "thin" $\tilde{Q}$, meaning only its first $n$ columns. Note that we can do this by applying the intermediate factors $\{Q_{r, k}\}$ to the first n columns of the $m \times m$ identity matrix. Write a Python script using MPI that does this (assume you are using 4 processors). (\textit{Hint: looking at the graphical representation of parallel TSQR might help.})

\textit{After performing the TSQR algorithm we end up with an implicit representation of } $Q,$ \textit{ call it } $\{Q_{r,k}$. \textit{This is an implicit tree representation of the orthogonal factor which is distributed across processors. For a given } $Q_{r, k}$ \textit{r indicates the processor number (both where it is computed and where it is stored), and k indicates the level in the tree. An algorithm to explicitly compute } $Q$ \textit{is given below. Note that this is an algorithm where we assume that we are given the implicit representation and we are using 4 processors.}

\lstinputlisting{../build/solution/buildQ.py}

    
    
 

    
\end{enumerate}

\end{document}
